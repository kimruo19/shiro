<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Copy Text Buttons</title>
    <style>
        button {
            margin: 5px;
        }

        body {
            background-color: black;
            /* Light gray background */
            font-family: Arial, sans-serif;
            text-align: center;
        }

        button {
            margin: 10px;
            padding: 10px;
            font-size: 16px;
            background-color: DimGray;
            /* Light gray button background */
            border: 1px solid #560b0b;
            /* Dark gray border */
            cursor: pointer;
        }

        button:hover {
            background-color: #ccc;
            /* Slightly darker background on hover */
        }
    </style>
</head>

<body>

    <button onclick="copyText(text1)">1-Spam Classifier</button>
    <button onclick=" copyText(text2)">2-Association-Apriori Algorithm</button>
    <button onclick="copyText(text3)">3-Crawler web search user defined</button>
    <button onclick="copyText(text4)">4-Sentiment Analysis </button>
    <button onclick="copyText(text5)">5-Link Analysis & PageRank</button>
    <button onclick="copyText(text6)">6-Scrape Online Social Media </button>
    <button onclick="copyText(text7)">7-Web Crawling & Indexing</button>
    <button onclick="copyText(text8)">8-Focused crawler-Reddit</button>
    
    
    <p id="copiedMsg"></p>
    <script>
        var text1 = `#Pract 1-Spam Classifier

        # Pract 1 - Perform Spam Classifier

# Spam Classifier

import tensorflow as tf
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

dataset = (
    "Hey, You got a lottery worth of $1000000!",
    "Can we have a lunch today?",
    "Congratulations, you won a prize in RummyCircle!",
    "An amount of $10000 has been credited to your Poker Account",
    "Your meeting will start at 5:00PM Today",
    "it's an important call for you"
)

label = (1, 0, 1, 1, 0, 0)

max_words, word_len = 1000, 50

tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
tokenizer.fit_on_texts(dataset)
sequences = tokenizer.texts_to_sequences(dataset)
padded_sequences = pad_sequences(sequences, maxlen=word_len, padding='post', truncating='post')


# Constructing a Neural Network

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=max_words, output_dim=16, input_length=word_len),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(16, activation='relu'), # adding hidden layer with 16 nodes (depend on output dimension)
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

training_sequence = np.array(padded_sequences)
training_label = np.array(label)

model.fit(training_sequence, training_label, epochs=10)

def check_is_spam(email, threshold=0.5):
    sequence = tokenizer.texts_to_sequences([email])
    padded_sequence = pad_sequences(sequence, maxlen=word_len, padding='post', truncating='post')
    prediction = model.predict(padded_sequence)[0][0]

    if prediction > threshold:
        print(f"Probably Spam mail, Score: {prediction:.2f}")
    else:
        print(f"Probably Not a Spam mail, Score: {prediction:.2f}")

#check_is_spam("hey, congratulations you won a lottery of $200")
check_is_spam("You have meeting today at 3:00 PM")

print("\nCoded By: Shipra Jana, Roll no. 014")

    `;

        var text2 = ` #Pract 2-Association-Apriori Algorithm

        # Pract 2 - Association Mining using Apriori Algorithm

        # Remove --> , num_itemsets='support' - this in Pycharm


# Import required libraries for association Mining using Apriori Algorithm
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules
import pandas as pd
print("***********************")
print("Shipra Jana 014")
print("***********************")

# Sample Transaction Dataset
dataset = [
           ['milk', 'bread', 'nuts'],
           ['milk', 'bread'],
           ['milk', 'eggs', 'nuts'],
           ['milk', 'bread', 'eggs'],
           ['bread', 'nuts'],
          ]
'''print("Sample Transaction Dataset")
print(dataset)'''

print("Sample Transaction Dataset:")
# Print each transaction on a new line
for transaction in dataset:
    print(transaction)

# Convert the dataset to a pandas dataframe
df = pd.DataFrame(dataset)
print("Transaction database: ")
print(df)

# Perform one-hot encoding(convert items to column)
df_encoded = pd.get_dummies(df, prefix='', prefix_sep='')
print("Transaction encoded: ")
print(df_encoded)

# Find frequent itemsets using Apriori
frequewnt_itemsets = apriori(df_encoded, min_support=0.5, use_colnames=True)
print("Frequent  itemsets: ")
print(frequewnt_itemsets)

# Generation Association Rules
rules = association_rules(frequewnt_itemsets, metric="lift", min_threshold=1, num_itemsets='support')
print("Association Rules: ")
print(rules)
print("*********************")
print("Shipra Jana 014")
print("*********************")

`;

        var text3 = ` # Practical No 3 - Crawler web search user defined

# https://www.shipra.in - use failed to retrive
# Develop a basic crawler for the web search for user defined keywords.

#Libraries Required
import requests
from bs4 import BeautifulSoup
import re

print("******************")
print("Shipra Jana - 014")
print("******************")


def crawl_and_search(url, keyword):
    try:
        # Fetch the content from the URL
        response = requests.get(url)
        response.raise_for_status()  # Check if the request was successful
        page_content = response.text

        # Parse the content using BeautifulSoup
        soup = BeautifulSoup(page_content, 'html.parser')

        # Extract the text from the page
        text = soup.get_text()

        # Search for the keyword in the text
        if re.search(keyword, text, re.IGNORECASE):
            print(f"Keyword '{keyword}' found in {url}")
        else:
            print(f"Keyword '{keyword}' not found in {url}")

    except requests.exceptions.RequestException as e:
        print(f"Failed to retrieve {url}: {e}")

# Define the URL and the keyword to search for
url = input("Enter the URL to crawl: ")
keyword = input("Enter the keyword to search for: ")

# Start the crawling process
crawl_and_search(url, keyword)

print("******************")
print("Shipra Jana - 014")
print("******************")
        
  `;
  
        var text4 = ` # Pract 4 - Sentiment Analysis 


# Sentiment analysis for reviews by customers and visualize the same.


import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import matplotlib.pyplot as plt
import seaborn as sns

print("*******************")
print("Shipra Jana 014")
print("*******************")

# Step 1: Download VADER lexicon if not already done
nltk.download('vader_lexicon')
# Step 2: Initialize the VADER sentiment intensity analyzer
sia = SentimentIntensityAnalyzer()
# Step 3: List of customer reviews
reviews = [
    "The product quality is amazing, I'm very satisfied!",
    "Terrible service, I will never buy from here again.",
    "Decent product, but shipping was too slow.",
    "Absolutely love it! Will recommend to everyone.",
    "Not worth the money, very disappointing.",
    "Great experience overall, but could improve the packaging.",
    "Mediocre, not what I expected.",
    "Excellent value for the price, highly recommended.",
    "Worst purchase I've made this year.",
    "It's okay, nothing special."
]
# Step 4: Analyze sentiment for each review
sentiments = []
for review in reviews:
    sentiment_score = sia.polarity_scores(review)
    compound_score = sentiment_score['compound']
    if compound_score >= 0.05:
        sentiments.append('Positive')
    elif compound_score <= -0.05:
        sentiments.append('Negative')
    else: sentiments.append('Neutral')
# Step 5: Count the occurrences of each sentiment
sentiment_counts = {
    'Positive': sentiments.count('Positive'),
     'Negative': sentiments.count('Negative'),
    'Neutral': sentiments.count('Neutral')
}

# Step 6: Visualization
sns.set(style="whitegrid")
plt.figure(figsize=(8, 6))
sns.barplot(x=list(sentiment_counts.keys()), y=list(sentiment_counts.values()),
            palette="viridis")
plt.title('\nKSMSCIT014 SHIPRA JANA \n\n Sentiment Analysis of Customer Reviews\n')
plt.xlabel('\nSentiment \n\n Shipra Jana 014\n')
plt.ylabel('Number of Reviews')
plt.show()


`;

        var text5 = `# Pract 5 - Link Analysis & PageRank


# Page Rank Algorithm | Apply the PageRank algorithm to a small web graph and analyze the results.

print("*********************")
print("Shipra Jana 014")
print("*********************")

# Import the necessary library
import networkx as nx

# Create a random directed graph with 'n' nodes, where each node has 'k' outgoing edges.
# The 'alpha' parameter determines the probability of connecting nodes.
G = nx.random_k_out_graph(n=8, k=2, alpha=0.75)

# Define a function to draw the graph with labels and bold font for clarity
def draw_graph(G):
    nx.draw(G, with_labels=True, font_weight='bold', node_size=400)

# Draw the generated graph
draw_graph(G)

# Calculate PageRank values for each node in the graph.
# The 'alpha' parameter (usually set to 0.85) represents the probability of continuing the walk to a random connected node.
ranks_pr = nx.pagerank(G=G, alpha=0.85)

'''# Print the PageRank values
print("Page Rank: ")
print(ranks_pr)'''

# Print the PageRank values line by line
print("Page Rank: ")
for node, rank in ranks_pr.items():
    print(f"Node {node}: {rank}")

print("*********************")
print("Shipra Jana 014")
print("*********************")


`;

        var text6 = `# Pract 6 - Scrape Online Social Media 

# Scrape an online Social Media Site for Data. Use Python to scrape information from any website.

# Import necessary libraries
import requests
from bs4 import BeautifulSoup

print("*******************")
print("Shipra Jana 014")
print("*******************")

# Define a function to scrape data from a website and check for a specific word
def check_word_in_webpage(url, word):
    # Send a GET request to the specified URL
    response = requests.get(url)

    # Check if the request was successful
    if response.status_code == 200:
        # Parse the webpage content using BeautifulSoup
        soup = BeautifulSoup(response.content, 'html.parser')

        # Extract all text content from the webpage
        text_content = soup.get_text()

        # Check if the specified word is present in the text content (case insensitive)
        if word.lower() in text_content.lower():
            print(f"The word '{word}' is present in the webpage.")
        else:
            print(f"The word '{word}' is not present in the webpage.")
    else:
        # Print an error message if the webpage could not be retrieved
        print("Failed to retrieve webpage.")

# Prompt the user to enter the URL of the webpage they want to scrape
url = input("Enter the URL you want to scrape: ")

# Prompt the user to enter the word they want to check for in the webpage
word_to_check = input("Enter the text you want to check for presence in the webpage: ")

# Call the function to scrape the webpage and check for the specified word
check_word_in_webpage(url, word_to_check)

print("*******************")
print("Shipra Jana 014")
print("*******************")

`;
   
        var text7 = `# Pract 7 - Web Crawling & Indexing


# Web Crawling and Indexing
# a) Develop a web crawler to fetch and index a web page.
# Handle challenges such as robots.txt, dynamic content and crawling delays.


# !pip install nltk  -- download
import nltk
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re


print("*********************")
print("Shipra Jana 014")
print("*********************")

# Ensure that the required NLTK data is downloaded
nltk.download('stopwords')
nltk.download('punkt')
# Download the 'punkt_tab' data package
nltk.download('punkt_tab')  # This line is added to fix the error

# Stop words (to filter out common words)
stop_words = set(stopwords.words('english'))

# Seed URLs (updated with real URLs for testing)
seed_urls = [
    'https://www.bbc.com/news',
    'https://edition.cnn.com',
    'https://www.theguardian.com',
    'https://www.yelp.com/search?find_desc=Restaurants&find_loc=New+York%2C+NY',
    'https://ny.eater.com'
]

# Keywords to focus on
keywords = ['restaurant', 'food', 'local']

# Visited URLs
visited = set()

def is_relevant(content, keywords):
    """
    Check if the content is relevant based on the keywords.
    """
    words = word_tokenize(content.lower())
    words = [w for w in words if w.isalnum() and w not in stop_words]
    return any(keyword in words for keyword in keywords)

def crawl(url):
    """
    Crawl a single webpage.
    """
    try:
        response = requests.get(url)
        response.raise_for_status()  # Raise an HTTPError for bad responses
        soup = BeautifulSoup(response.content, 'html.parser')
        text = soup.get_text()

        # Check if the content is relevant
        if is_relevant(text, keywords):
            print(f"Relevant content found at: {url}")
            # Here you could save the content to a file or database

        # Extract links and follow them
        for link in soup.find_all('a', href=True):
            new_url = urljoin(url, link['href'])
            if new_url not in visited and re.match(r'^https?://', new_url):
                visited.add(new_url)
                crawl(new_url)
    except requests.exceptions.RequestException as e:
        print(f"Error crawling {url}: {e}")

# Start crawling from the seed URLs
for url in seed_urls:
    if url not in visited:
        visited.add(url)
        crawl(url)

print("*********************")
print("Shipra Jana 014")
print("*********************")




`;

var text8 = `# Pract 8 - Focused crawler-Reddit


# 1)	Create a REDDIT Account First.
# 2)	Go to Reddit /Preferences /Apps.  https://www.reddit.com/prefs/apps
# 3)	Create an application.
# 4)	Client_id, Client_secret will be generated.
# Client_id - PchAOkUNcBteGiXsjrGiSg  |  Client_secret - QyzHLj77qxdx1zwNM7flUjRsbj6bdQ


# Develop a focused crawler for local search(Use Reddit).

#User defined keyword webcrawling- Reddit Subs
#Libraries Required
# pip install praw  - install

import praw
import pandas as pd

print("******************")
print("Shipra Jana - 014")
print("******************")

#https://www.youtube.com/watch?v=NRgfgtzIhBQ
#Watch start 4 mins to get client_id,client_secret,user_agent
#after adding above mentioned than only it will work

reddit = praw.Reddit(client_id='PchAOkUNcBteGiXsjrGiSg', #removed trailing space
                     client_secret='QyzHLj77qxdx1zwNM7flUjRsbj6bdQ', #removed trailing space
                     user_agent='Shipra_014')

sub_name = input("enter the Keyword - ")
max_posts = 5

# Enable read-only mode
reddit.read_only = True

title=[]

for submission in reddit.subreddit(sub_name).new(limit=max_posts):
 title.append(submission.title)

print(title)

df = pd.DataFrame(title,columns=['Title'])
print(df)

print("******************")
print("Shipra Jana - 014")
print("******************")


`;



function copyText(text) {
    navigator.clipboard.writeText(text).then(function () {
        document.getElementById('copiedMsg').innerHTML = "Text Copied"
    }).catch(function (err) {
        console.error('Unable to copy text', err);
    });
}
</script>

</body>

</html>
